{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Automatically subtitle a Dhivehi Video\n",
    "\n",
    "This is a simple demonstration of a use case for an ASR toolchain,\n",
    "such as the Hugging Face wav2vec2 model mentioned on\n",
    "[https://dhivehi.ai/docs/technologies/stt/](https://dhivehi.ai/docs/technologies/stt/)\n",
    "\n",
    "The tutorial is inspired by [this article](https://towardsdatascience.com/generating-subtitles-automatically-using-mozilla-deepspeech-562c633936a7)\n",
    "published towardsdatascience.com.\n",
    "\n",
    "The process follows a few basic steps:\n",
    " * Extract audio from the video\n",
    " * Download STT pretrained model and setup inference pipeline\n",
    " * Run STT on the audio to transcribe the audio\n",
    " * generate a .srt file containing subtitles with timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup requirements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install ../requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process and extract audio\n",
    "\n",
    "For the purposes of this tutorial, we will be using this episode of Floak the International\n",
    "downloaded off Youtube [here](https://www.youtube.com/watch?v=ccdwQQ1OQB4)\n",
    "\n",
    "Before you proceed, download your video and store it somewhere."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### define some helper methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pyAudioAnalysis.audioSegmentation import silence_removal\n",
    "from pyAudioAnalysis.audioBasicIO import read_audio_file\n",
    "from scipy.io import wavfile\n",
    "\n",
    "def extractAudio(input_file, audio_file_name):\n",
    "    command = \"ffmpeg -hide_banner -loglevel warning -i {} -b:a 192k -ac 1 -ar 16000 -vn {}\".format(input_file, audio_file_name)\n",
    "    try:\n",
    "        ret = subprocess.call(command, shell=True)\n",
    "        print(\"Extracted audio to audio/{}\".format(audio_file_name.split(\"/\")[-1]))\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", str(e))\n",
    "        exit(1)\n",
    "\n",
    "def silenceRemoval(input_file, output_dir, smoothing_window = 1.0, weight = 0.1):\n",
    "    print(\"Detecting silences...\")\n",
    "    [fs, x] = read_audio_file(input_file)\n",
    "    segmentLimits = silence_removal(x, fs, 0.05, 0.05, smoothing_window, weight)\n",
    "    ifile_name = os.path.basename(input_file)\n",
    "\n",
    "    print(\"Writing segments...\")\n",
    "    for i, s in enumerate(segmentLimits):\n",
    "        strOut = \"{0:s}_{1:.3f}-{2:.3f}.wav\".format(ifile_name, s[0], s[1])\n",
    "        strOut = os.path.join(output_dir, strOut)\n",
    "        wavfile.write(strOut, fs, x[int(fs * s[0]):int(fs * s[1])])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Extract the audio"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted audio to audio/floak_ep1.wav\n",
      "Detecting silences...\n",
      "Writing segments...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"../audio/segments/\")\n",
    "extractAudio(\"../floak_ep1.mp4\", \"../audio/floak_ep1.wav\")\n",
    "silenceRemoval(\"../audio/floak_ep1.wav\", \"../audio/segments/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup the STT pipeline\n",
    "\n",
    "For this tutorial, we will be using the minimal quantized model\n",
    "to run inference. If you are looking to use the full model for extra\n",
    "fine-tuning, refer to the [Hugging Face Model page](https://huggingface.co/shahukareem/wav2vec2-large-xlsr-53-dhivehi)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download the STT model and extract it somewhere"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os\n",
    "from shutil import unpack_archive\n",
    "\n",
    "# set output dir\n",
    "op_dir = \"../models\"\n",
    "op_file = os.path.join(op_dir, \"w2v2-53.tar.gz\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # download and extract\n",
    "os.makedirs(op_dir, exist_ok=True)\n",
    "gdown.download(\n",
    "    f\"https://drive.google.com/uc?id=1m6QXhMF9Zf6P04Z1D2qFiQjEFo16Vexv\",\n",
    "    op_file\n",
    ")\n",
    "unpack_archive(op_file, \"../models\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize the STT model and prepare it for inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer, Wav2Vec2Processor\n",
    "\n",
    "model_dir = os.path.join(op_dir, \"stt_model\")\n",
    "STT_MODEL_PATH = os.path.join(model_dir, \"wav2vec_traced_quantized.pt\")\n",
    "STT_VOCAB_FILE = os.path.join(model_dir, \"vocab.json\")\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(STT_VOCAB_FILE, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=SAMPLING_RATE, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "model = torch.jit.load(STT_MODEL_PATH)\n",
    "\n",
    "def transcribe(audio_path):\n",
    "    audio_input, sr = librosa.load(audio_path, sr=SAMPLING_RATE)\n",
    "    inputs = processor(\n",
    "        audio_input,\n",
    "        sampling_rate=SAMPLING_RATE,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values)['logits']\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "    return transcription\n",
    "\n",
    "\n",
    "def process_audio(audio_file):\n",
    "    start, end = audio_file.split(\"/\")[-1][:-4].split(\"_\")[-1].split(\"-\")\n",
    "    transcription = transcribe(audio_file)\n",
    "    return start,end,transcription"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transcribe the audio to subtitles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [05:50<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "def write_to_file(file_handle, inferred_text, line_count, limits):\n",
    "    \"\"\"Write the inferred text to SRT file\n",
    "    Follows a specific format for SRT files\n",
    "    Args:\n",
    "        file_handle : SRT file handle\n",
    "        inferred_text : text to be written\n",
    "        line_count : subtitle line count\n",
    "        limits : starting and ending times for text\n",
    "    \"\"\"\n",
    "\n",
    "    d = str(datetime.timedelta(seconds=float(limits[0])))\n",
    "    try:\n",
    "        from_dur = \"0\" + str(d.split(\".\")[0]) + \",\" + str(d.split(\".\")[-1][:2])\n",
    "    except:\n",
    "        from_dur = \"0\" + str(d) + \",\" + \"00\"\n",
    "\n",
    "    d = str(datetime.timedelta(seconds=float(limits[1])))\n",
    "    try:\n",
    "        to_dur = \"0\" + str(d.split(\".\")[0]) + \",\" + str(d.split(\".\")[-1][:2])\n",
    "    except:\n",
    "        to_dur = \"0\" + str(d) + \",\" + \"00\"\n",
    "\n",
    "    file_handle.write(str(line_count) + \"\\n\")\n",
    "    file_handle.write(from_dur + \" --> \" + to_dur + \"\\n\")\n",
    "    file_handle.write(inferred_text + \"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../floak_ep1.srt\", \"w\") as f:\n",
    "\n",
    "    for w_file in tqdm(glob(\"../audio/segments/*.wav\")):\n",
    "        start, end, transcription = process_audio(w_file)\n",
    "        if len(transcription.strip())==0:\n",
    "            continue\n",
    "\n",
    "        write_to_file(f, transcription, 1, (start, end))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}